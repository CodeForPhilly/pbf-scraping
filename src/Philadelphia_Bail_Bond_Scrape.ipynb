{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping for the Philadelphia Bail Bond\n",
    "\n",
    "This code will scrape data from the Philadelphia Courts, cleans the data, and outputs a CSV file. Future implementation is to have it check pages on its own, but for now manual entry of end page is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # This list will hold the scraped data from each page\n",
    "    scraped_list_per_page = []\n",
    "    # The current page is 1 and the end page as of now is 3 (this needs to be manually checked)\n",
    "    curr_page_num, end_page = (1,3)\n",
    "    # Starting at the current page and stopping at the last page of the website\n",
    "    for curr_page_num in range(end_page):\n",
    "        # Take the current page number and increament it each iteration\n",
    "        curr_page_num = 1 + curr_page_num\n",
    "        # The current webpage stores up to 24 criminal files and we are going through each page by updating the page number in the format\n",
    "        curr_page = \"https://www.courts.phila.gov/NewCriminalFilings/date/default.aspx?search=2020-06-12&searchdt=&searchtype=&page={}\".format(curr_page_num)\n",
    "        # Then get the HTML file of the page as text\n",
    "        source = requests.get(curr_page).text\n",
    "        # Then create a BeautifulSoup object of the text, this makes pulling data out of HTML files easier\n",
    "        # To learn more about it read here (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "        soup = BeautifulSoup(source)\n",
    "        # After inspecting the source code I noticed the criminal files were listed under this specific div tag\n",
    "        # The findAll function will grab each criminal file from that page\n",
    "        list_of_criminal_filings = soup.findAll(\"div\", {\"class\": \"well well-sm\"})\n",
    "        # Then pass the list of all criminal fiilings into the extract_attributes function\n",
    "        # After the extract_attributes function completes it will return a list of that whole page's scraped criminal\n",
    "        # filings and then it will continue to the next page and at the end we will have one complete joined list\n",
    "        scraped_list_per_page = (extract_attributes(list_of_criminal_filings)) + scraped_list_per_page\n",
    "    # The joined list will then be passed into the create_csv function and converted to CSV\n",
    "    create_csv(scraped_list_per_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_attributes(list_of_criminal_filings):\n",
    "    list_of_criminal_file_scraped = []\n",
    "    # For each criminal file in the list of criminal filings pass it into the scrape_and_store function\n",
    "    # Then afterwards return everything to main and it will repeat this cycle for the amount of pages\n",
    "    for criminal_file in list_of_criminal_filings:\n",
    "        criminal_file_scraped = scrape_and_store(criminal_file.text)\n",
    "        list_of_criminal_file_scraped.append(criminal_file_scraped)\n",
    "    return list_of_criminal_file_scraped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just regex functions that helped me clean the data you can read more about regex here (https://docs.python.org/3/library/re.html)\n",
    "def scrape_and_store(text):\n",
    "    hold = text.splitlines()\n",
    "    defendant_name = re.split('Name (.*?)', hold[3])[-1]\n",
    "    age = re.split('Age (.*?)', hold[4])[-1]\n",
    "    address = hold[6]\n",
    "    city = re.split('\\t ', address.split(',')[0])[1]\n",
    "    state = re.split(\" (.*?) \", re.split(\",\", address)[1])[1]\n",
    "    zip_code = re.split(\" (.*?) \", re.split(\",\", address)[1])[2]\n",
    "    docket_number = re.split(\"Number (.*?)\", hold[11])[2]\n",
    "    filing = re.split(\" \", hold[12])\n",
    "    filing_date = filing[2]\n",
    "    filing_time = \" \".join(filing[3:5])\n",
    "    charge = re.split(\"Charge \", hold[13])[1]\n",
    "    represented = hold[15].strip()\n",
    "    in_custody = hold[16]\n",
    "    if len(in_custody) != 1:\n",
    "        try:\n",
    "            in_custody = re.split(\"Custody (.*?)\", in_custody)[2]\n",
    "        except IndexError as error:\n",
    "            in_custody = \"\"\n",
    "    bail_status = re.split(\"\\t(.*?)\", hold[-10])[-1]\n",
    "    bail_datetime = re.split(\" \", hold[-9])\n",
    "    bail_date = bail_datetime[2]\n",
    "    bail_time = \" \".join(bail_datetime[3:5])\n",
    "    bail_type = re.split(\": (.*?)\", hold[-8])[-1]\n",
    "    bail_amount = re.split(\": (.*?)\", hold[-7])[-1]\n",
    "    outstanding_bail_amt = re.split(\" \", hold[-6])[-1]\n",
    "    # Return a list of all the attributes\n",
    "    return [defendant_name, age, city, state, zip_code, docket_number, filing_date, filing_time, charge, represented, in_custody, bail_status, bail_date, bail_time, bail_type, bail_amount, outstanding_bail_amt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will make the list of lists into a CSV file with Pandas\n",
    "def create_csv(list_of_criminal_file_scraped):\n",
    "    df = pd.DataFrame(list_of_criminal_file_scraped)\n",
    "    df.to_csv(\"output.csv\", index=False, header=[\"Defendant Name\", \"Age\", \"City\", \"State\", \"Zip Code\", \"Docket Number\", \"Filing Date\", \"Filing Time\", \"Charge\", \"Represented\", \"In Custody\", \"Bail Status\", \"Bail Date\", \"Bail Time\", \"Bail Type\", \"Bail Amount\", \"Outstanding Bail Amount\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
